{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "\n",
    "def predict(X, parameters):\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [12288, 1])\n",
    "    \n",
    "    z3 = forward_propagation_for_predict(x, params)\n",
    "    p = tf.argmax(z3)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "def forward_propagation_for_predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3'] \n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = pd.read_table('C:/Users/gokul/Documents/Projects/human Activity/X_train.txt', delim_whitespace=True,header=None)\n",
    "trainy = pd.read_table('C:/Users/gokul/Documents/Projects/human Activity/y_train.txt',delim_whitespace=True,header=None)\n",
    "testX = pd.read_csv(\"C:/Users/gokul/Documents/Projects/human Activity/X_test.txt\",delim_whitespace=True,header=None)\n",
    "testy = pd.read_csv('C:/Users/gokul/Documents/Projects/human Activity/y_test.txt',delim_whitespace=True,header=None)\n",
    "\n",
    "permtrain = np.random.permutation(7352)\n",
    "permtest = np.random.permutation(2947)\n",
    "testy = testy.iloc[permtest]\n",
    "testX = testX.iloc[permtest]\n",
    "trainX = trainX.iloc[permtrain]\n",
    "trainy = np.array(trainy.iloc[permtrain])\n",
    "\n",
    "mu = trainX.mean(axis=0)\n",
    "mu1 = testX.mean(axis=0)\n",
    "stdv = trainX.std(axis = 0)\n",
    "stdv1 = testX.std(axis = 0)\n",
    "\n",
    "X = (trainX - mu)/stdv\n",
    "X_test = (testX - mu1)/stdv1\n",
    "X = X.T\n",
    "\n",
    "num_labels = np.unique(trainy).shape[0]\n",
    "Y_train = np.zeros((trainy.shape[0],num_labels))\n",
    "Y_train[np.arange(7352), np.array(trainy-1).flatten()] = 1\n",
    "Y_test = np.zeros((testy.shape[0],num_labels))\n",
    "Y_test[np.arange(2947), np.array(testy-1).flatten()] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X:0\", shape=(12288, ?), dtype=float32)\n",
      "Y = Tensor(\"Y:0\", shape=(6, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(shape=[n_x,None], dtype= \"float\", name = \"X\")\n",
    "    Y = tf.placeholder(shape=[n_y, None], dtype = \"float\", name = \"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X, Y = create_placeholders(12288, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(100, 561) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(100, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(25, 100) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(25, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    W1 = tf.get_variable(\"W1\", [100, 561], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [100,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [25, 100], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [25,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [6, 25], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [6,1], initializer = tf.zeros_initializer())\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1 )                                             # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2 )                                               # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3 )                                               # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Z3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits,labels = labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 200, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X,parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3,Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561, 7352)\n",
      "Cost after epoch 0: 1.091805\n",
      "Cost after epoch 100: 0.011250\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDxLAzpV/5RRVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181a1dd6470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.951476\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(trainX.T)\n",
    "X_test = np.array(testX.T)\n",
    "print(X_train.shape)\n",
    "parameters = model(X_train, Y_train.T, X_test, Y_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
