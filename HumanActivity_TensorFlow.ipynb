{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "\n",
    "def predict(X, parameters):\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [12288, 1])\n",
    "    \n",
    "    z3 = forward_propagation_for_predict(x, params)\n",
    "    p = tf.argmax(z3)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "def forward_propagation_for_predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3'] \n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = pd.read_table('C:/Users/gokul/Documents/Projects/human Activity/X_train.txt', delim_whitespace=True,header=None)\n",
    "trainy = pd.read_table('C:/Users/gokul/Documents/Projects/human Activity/y_train.txt',delim_whitespace=True,header=None)\n",
    "testX = pd.read_csv(\"C:/Users/gokul/Documents/Projects/human Activity/X_test.txt\",delim_whitespace=True,header=None)\n",
    "testy = pd.read_csv('C:/Users/gokul/Documents/Projects/human Activity/y_test.txt',delim_whitespace=True,header=None)\n",
    "\n",
    "permtrain = np.random.permutation(7352)\n",
    "permtest = np.random.permutation(2947)\n",
    "testy = testy.iloc[permtest]\n",
    "testX = testX.iloc[permtest]\n",
    "trainX = trainX.iloc[permtrain]\n",
    "trainy = np.array(trainy.iloc[permtrain])\n",
    "\n",
    "mu = trainX.mean(axis=0)\n",
    "mu1 = testX.mean(axis=0)\n",
    "stdv = trainX.std(axis = 0)\n",
    "stdv1 = testX.std(axis = 0)\n",
    "\n",
    "X = (trainX - mu)/stdv\n",
    "X_test = (testX - mu1)/stdv1\n",
    "X = X.T\n",
    "\n",
    "num_labels = np.unique(trainy).shape[0]\n",
    "Y_train = np.zeros((trainy.shape[0],num_labels))\n",
    "Y_train[np.arange(7352), np.array(trainy-1).flatten()] = 1\n",
    "Y_test = np.zeros((testy.shape[0],num_labels))\n",
    "Y_test[np.arange(2947), np.array(testy-1).flatten()] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X:0\", shape=(12288, ?), dtype=float32)\n",
      "Y = Tensor(\"Y:0\", shape=(6, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(shape=[n_x,None], dtype= \"float\", name = \"X\")\n",
    "    Y = tf.placeholder(shape=[n_y, None], dtype = \"float\", name = \"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X, Y = create_placeholders(12288, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(100, 561) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(100, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(25, 100) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(25, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    W1 = tf.get_variable(\"W1\", [100, 561], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [100,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [25, 100], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [25,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [6, 25], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [6,1], initializer = tf.zeros_initializer())\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1 )                                             # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2 )                                               # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3 )                                               # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Z3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits,labels = labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 200, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X,parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3,Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "#             print(len(minibatches))\n",
    "#             break\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561, 7352)\n",
      "230\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158d4baacc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.128264\n",
      "Test Accuracy: 0.147947\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(trainX.T)\n",
    "X_test = np.array(testX.T)\n",
    "print(X_train.shape)\n",
    "parameters = model(X_train, Y_train.T, X_test, Y_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
