{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "\n",
    "def predict(X, parameters):\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [12288, 1])\n",
    "    \n",
    "    z3 = forward_propagation_for_predict(x, params)\n",
    "    p = tf.argmax(z3)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "def forward_propagation_for_predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3'] \n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pd.read_table('C:/Users/gokul/Documents/Projects/human Activity/X_train.txt', delim_whitespace=True,header=None)\n",
    "trainy = pd.read_table('C:/Users/gokul/Documents/Projects/human Activity/y_train.txt',delim_whitespace=True,header=None)\n",
    "testX = pd.read_csv(\"C:/Users/gokul/Documents/Projects/human Activity/X_test.txt\",delim_whitespace=True,header=None)\n",
    "testy = pd.read_csv('C:/Users/gokul/Documents/Projects/human Activity/y_test.txt',delim_whitespace=True,header=None)\n",
    "\n",
    "permtrain = np.random.permutation(7352)\n",
    "permtest = np.random.permutation(2947)\n",
    "testy = testy.iloc[permtest]\n",
    "testX = testX.iloc[permtest]\n",
    "trainX = trainX.iloc[permtrain]\n",
    "trainy = np.array(trainy.iloc[permtrain])\n",
    "\n",
    "mu = trainX.mean(axis=0)\n",
    "mu1 = testX.mean(axis=0)\n",
    "stdv = trainX.std(axis = 0)\n",
    "stdv1 = testX.std(axis = 0)\n",
    "\n",
    "\n",
    "X = (trainX - mu)/stdv\n",
    "X_test = (testX - mu1)/stdv1\n",
    "X = X.T\n",
    "\n",
    "# def convert_to_one_hot(Y, C):\n",
    "#     Y = np.eye(C)[Y.reshape(-1)].T\n",
    "#     return Y\n",
    "\n",
    "# Y = convert_to_one_hot(trainy, 5)\n",
    "# print(Y.shape)\n",
    "num_labels = np.unique(trainy).shape[0]\n",
    "Y_train = np.zeros((trainy.shape[0],num_labels))\n",
    "Y_train[np.arange(7352), np.array(trainy-1).flatten()] = 1\n",
    "Y_test = np.zeros((testy.shape[0],num_labels))\n",
    "Y_test[np.arange(2947), np.array(testy-1).flatten()] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 6)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X:0\", shape=(12288, ?), dtype=float32)\n",
      "Y = Tensor(\"Y:0\", shape=(6, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(shape=[n_x,None], dtype= \"float\", name = \"X\")\n",
    "    Y = tf.placeholder(shape=[n_y, None], dtype = \"float\", name = \"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X, Y = create_placeholders(12288, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(100, 561) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(100, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(25, 100) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(25, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    W1 = tf.get_variable(\"W1\", [100, 561], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [100,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [25, 100], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [25,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [6, 25], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [6,1], initializer = tf.zeros_initializer())\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = Tensor(\"Add_2:0\", shape=(6, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1 )                                             # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2 )                                               # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3 )                                               # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Z3\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(12288, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits,labels = labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X,parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3,Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561, 7352)\n",
      "Cost after epoch 0: 1.093884\n",
      "Cost after epoch 100: 0.013789\n",
      "Cost after epoch 200: 0.000966\n",
      "Cost after epoch 300: 0.000128\n",
      "Cost after epoch 400: 0.000089\n",
      "Cost after epoch 500: 0.000090\n",
      "Cost after epoch 600: 0.000008\n",
      "Cost after epoch 700: 0.003498\n",
      "Cost after epoch 800: 0.000010\n",
      "Cost after epoch 900: 0.008512\n",
      "Cost after epoch 1000: 0.000001\n",
      "Cost after epoch 1100: 0.000001\n",
      "Cost after epoch 1200: 0.000000\n",
      "Cost after epoch 1300: 0.000001\n",
      "Cost after epoch 1400: 0.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOBTMzKzAoWBmZgUOBTMzK3AomJlZwX8D6UBsLkSuiRQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a79c55e160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.958941\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(trainX.T)\n",
    "X_test = np.array(testX.T)\n",
    "print(X_train.shape)\n",
    "parameters = model(X_train, Y_train.T, X_test, Y_test.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
